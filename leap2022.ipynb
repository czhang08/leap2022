{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkSGVSxZDrx8"
      },
      "source": [
        "Tips:\n",
        "\n",
        "* Enable a GPU in Colab before running the neural network examples in this notebook. *Edit -> Notebook settings -> Hardware accelerator -> GPU.* \n",
        "\n",
        "* Should you need to reset your environment to a clean state, you can use *Runtime -> Disconnect and delete runtime*.\n",
        "\n",
        "* You can find GPU info by running `!nvidia-smi`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v5mm4amQRrm"
      },
      "source": [
        "# LEAP, May 2022 üåé: Trees and Neural Networks with TensorFlow \n",
        "\n",
        "Welcome! Today, you'll gain hands-on experience training decision forests and neural networks with TensorFlow. Trees and neural networks are two of the most [frequently](https://www.kaggle.com/kaggle-survey-2021) used models on [Kaggle](https://kaggle.com/). Together, they provide an intro to practical ML. \n",
        "\n",
        "This notebook contains tutorials and exercises to help you get started. Your instructor will guide you through the sections you'll explore today. \n",
        "\n",
        "If you're new to deep learning (or ML in general), this is a *lot* of material to cover in a short workshop. Our goals are to dive in and get started. \n",
        "\n",
        "Here's an outline of what we'll cover:\n",
        "\n",
        "1. **Decision Forests**: You'll train a random forest on a tabular dataset that you load from a CSV file. This is a common pattern in practice. As an exercise, you'll train a gradient boosted tree.\n",
        "\n",
        "1. **Deep Neural Networks:** You'll train a DNN to classify handwritten digits. This is the \"hello world\" of computer vision, and a great place to begin if you're new to the subject. As an exercise, you'll experiment with a different dataset, and modify the network to improve the accuracy.\n",
        "\n",
        "1. **Transfer Learning:** You will download a pretrained model, use it to classify images, then add a classification head for new dataset.\n",
        "\n",
        "1. **Convolutional Neural Networks:** You'll write a CNN to classify images of cats and dogs, using a real-world dataset you load from a directory on disk. As an exercise, you'll use data augmentation and dropout to reduce overfitting.\n",
        "\n",
        "1. **DeepDream:** Deep learning is an amazingly wide field. If time remains, your instructor will walk you through DeepDream, which is a different flavor than a classification or regression task. \n",
        "\n",
        "Okay, let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpXrSP3bLltB"
      },
      "source": [
        "# Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üå≤üå≥üå≤üå≥üå≤üêøÔ∏èüêª"
      ],
      "metadata": {
        "id": "G8cL-suNMJmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Forests are a family of tree-based models including Random Forests and Gradient Boosted Trees. They are the most popular class of models on Kaggle, and are the best place to start when working with tabular data.\n",
        "\n",
        "You will use TensorFlow to train each of these on a dataset you load from a CSV file. This is a common pattern in practice. Roughly, your code will look as follows:\n",
        "\n",
        "```\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import pandas as pd\n",
        "  \n",
        "dataset = pd.read_csv(\"project/dataset.csv\")\n",
        "tf_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(dataset, label=\"my_label\")\n",
        "\n",
        "model = tfdf.keras.RandomForestModel()\n",
        "model.fit(tf_dataset)\n",
        "  \n",
        "print(model.summary())\n",
        "```"
      ],
      "metadata": {
        "id": "Z4eo3rH_MKbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install software\n",
        "\n",
        "There are many excellent libraries for working with tree-based models, including [scikit-learn](https://scikit-learn.org/) (highly recommended for all your ML needs), XGBoost, LightGBM, and others.\n",
        "\n",
        "Today, you'll use [TensorFlow Decision Forests (TF-DF)](https://www.tensorflow.org/decision_forests), a  library used in production at Google to train large models. The open-source release is currently in beta. \n",
        "\n",
        "If you use TF-DF in your climate research, we would *love* to hear about it. And/or, if you encounter bugs or friction not mentioned in the notes here, please email Josh.\n",
        "\n",
        "Let's install TF-DF now."
      ],
      "metadata": {
        "id": "yjIk3fktvokL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sfVlsJENfzS"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_decision_forests --quiet --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the library"
      ],
      "metadata": {
        "id": "FVOXAyXl3-fA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may see a warnings about certain distrubuted training modes not being available during the beta. That's expected, and you can safely ignore these."
      ],
      "metadata": {
        "id": "8pk0VHfqnc22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "IGmyjJJatzBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh4qwB4iN7Ue"
      },
      "outputs": [],
      "source": [
        "print(\"TensorFlow v\" + tf.__version__)\n",
        "print(\"TensorFlow Decision Forests v\" + tfdf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the penguins dataset\n",
        "\n",
        "To start, you will work with a small tabular [dataset](https://allisonhorst.github.io/palmerpenguins/articles/intro.html) of about 300 penguins. You will predict the species of penguin (Adelie, Gentoo, or Chinstrap) based on numeric attributes like their flipper length, and categorical attributes like the name of the island they're found on."
      ],
      "metadata": {
        "id": "-3vxMmCPvqpf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVMPH_IDOBH2"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!wget -q https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins.csv -O /tmp/penguins.csv\n",
        "\n",
        "# Load a dataset into a Pandas Dataframe\n",
        "dataset_df = pd.read_csv(\"/tmp/penguins.csv\")\n",
        "\n",
        "# Display the first 3 examples\n",
        "dataset_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the dataset"
      ],
      "metadata": {
        "id": "H4O7QCoh5e2e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-ftBmKgRsZB"
      },
      "source": [
        "This dataset contains a mix of numeric (*bill_depth_mm*), categorical (*island*) and missing features. TF-DF supports all these feature types natively, and no preprocessing is required. This is one of the advantages of tree-based models, and why they're a great place to start.\n",
        "\n",
        "You will have to slightly adjusted the labels, though, to convert them into the integer format TF-DF expects. The label (species) is stored as a string, so let's convert that into an integer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r3n7NOVRlx5"
      },
      "outputs": [],
      "source": [
        "label = \"species\"\n",
        "\n",
        "classes = dataset_df[label].unique().tolist()\n",
        "print(f\"Label classes: {classes}\")\n",
        "\n",
        "dataset_df[label] = dataset_df[label].map(classes.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brbRsBQfSC74"
      },
      "source": [
        "Next, split the dataset into training and testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsQad0t7SBv2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def split_dataset(dataset, test_ratio=0.30):\n",
        "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
        "  return dataset[~test_indices], dataset[test_indices]\n",
        "\n",
        "train_ds_pd, test_ds_pd = split_dataset(dataset_df)\n",
        "print(\"{} examples in training, {} examples in testing.\".format(\n",
        "    len(train_ds_pd), len(test_ds_pd)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hNGPbLlSGvp"
      },
      "source": [
        "There's one more step required before you can train your model. You need to convert from Pandas format (`pd.DataFrame`) into TensorFlow format (`tf.data.Dataset`). We've provided a single line helper function that will do this for you: \n",
        "\n",
        "```\n",
        "tfdf.keras.pd_dataframe_to_tf_dataset(your_df, label='species')\n",
        "```\n",
        "\n",
        "This is a high [performance](https://www.tensorflow.org/guide/data_performance) data loading library which is helpful when training neural networks with accelerators like GPUs and TPUs. It it not necessary for tree-based models until you begin to do distributed training - but we'll use it today for practice.\n",
        "\n",
        "Creating a fast input pipeline is important when working with neural networks, and forgetting to do so is the most common bug new researchers encounter. The author of this notebook has seen many folks with expensive GPUs that are idle ~50% of the time while waiting for data.\n",
        "\n",
        "Note that tf.data is a bit tricky to use, and has a learning curve. There are guides on [tensorflow.org/guide](https://www.tensorflow.org/guide) to help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQgimfirSGQ9"
      },
      "outputs": [],
      "source": [
        "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label)\n",
        "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label=label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUG4UKUyTNUu"
      },
      "source": [
        "## What models are available?\n",
        "\n",
        "There are several tree-based models for you to choose from. To start, you'll work with a Random Forest. Thus is the most well-known of the Decision Forest training algorithms. \n",
        "\n",
        "A Random Forest is a collection of decision trees, each trained independently on a random subset of the training dataset (sampled with replacement). The algorithm is unique in that it is robust to overfitting, and easy to use."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfdf.keras.get_all_models()"
      ],
      "metadata": {
        "id": "MFmnkRR_Ui9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike neural networks, decision forests have relatively few (and easy to configure) hyperparameters with good defaults."
      ],
      "metadata": {
        "id": "ddQP-nGY9Qeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How can I configure them?\n",
        "\n",
        "TF-DF provides good defaults for you (e.g. the top ranking hyperparameters on our benchmarks, slightly modified to run in reasonable time). You will use these defaults below. If you would like to configure the learning algorithm, you will find many options you can explore to get the highest possible accuracy. \n",
        "\n",
        "Let's check out the help on the ```RandomForestModel``` to see the options."
      ],
      "metadata": {
        "id": "LiFn716FnMVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(tfdf.keras.RandomForestModel)"
      ],
      "metadata": {
        "id": "BscU-0B3nGU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are **many** hyperparamters you can explore to grow exactly the type of forest you like. \n"
      ],
      "metadata": {
        "id": "GXLABIGERDEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can the parameters as follows\n",
        "print(tfdf.keras.RandomForestModel.predefined_hyperparameters())"
      ],
      "metadata": {
        "id": "Hr8k4TE8RQ62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can select a template and/or set parameters as follows:\n",
        "\n",
        "```gbt = tfdf.keras.GradientBoostedTreesModel(hyperparameter_template=\"benchmark_rank1\",num_trees=300)```\n"
      ],
      "metadata": {
        "id": "irxAS91IRVAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Random Forest \n",
        "\n",
        "Today, you will use the defaults. Let's create your model. "
      ],
      "metadata": {
        "id": "AUt4j8fLWRlR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7bqOQMYTRXZ"
      },
      "outputs": [],
      "source": [
        "rf = tfdf.keras.RandomForestModel()\n",
        "rf.compile(metrics=[\"accuracy\"]) # Optional, you can use this to include a list of eval metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train your model\n",
        "\n",
        "This is a one-liner.\n",
        "\n",
        "Note: you may see a warning about Autograph. You can safely ignore this, it will be fixed in the next release."
      ],
      "metadata": {
        "id": "0CzJ5_sh91Yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf.fit(x=train_ds)"
      ],
      "metadata": {
        "id": "Ax6RircN92LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1HJ6KxRT7IR"
      },
      "source": [
        "## Visualize your model\n",
        "One benefit of tree-based models is that you can easily visualize them. The default number of trees used in the Random Forest is 300. You can select a tree to display below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTx73NgET9f8"
      },
      "outputs": [],
      "source": [
        "tfdf.model_plotter.plot_model_in_colab(rf, tree_idx=0, max_depth=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fazbJOgUT1n4"
      },
      "source": [
        "## Evaluate the model on OOB data and the test dataset\n",
        "\n",
        "Let's plot accuracy on OOB evaluation dataset as a function of the number of trees in the forest. One of the nice features about this particular hyperparameter is that larger values are usually better, and come with little risk aside from slowing down training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryddKoqLWrTp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "logs = rf.make_inspector().training_logs()\n",
        "plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"Accuracy (out-of-bag)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also see some general stats on the OOB dataset:"
      ],
      "metadata": {
        "id": "Y-yMMsK5-3Mr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdY8DvriTxky"
      },
      "outputs": [],
      "source": [
        "inspector = rf.make_inspector()\n",
        "inspector.evaluation()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's run an evaluation using the test data. Depending on the random split your accuracy will likely between 90-100%."
      ],
      "metadata": {
        "id": "GAoGJNjg-9sb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39x97YqWZlgm"
      },
      "outputs": [],
      "source": [
        "evaluation = rf.evaluate(x=test_ds,return_dict=True)\n",
        "\n",
        "for name, value in evaluation.items():\n",
        "  print(f\"{name}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variable importances\n",
        "\n",
        "There are several ways to identify important features. Let's see the available options:"
      ],
      "metadata": {
        "id": "LWWqqDLM7WdZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xok16_jMgGZH"
      },
      "outputs": [],
      "source": [
        "print(f\"Available variable importances:\")\n",
        "for importance in inspector.variable_importances().keys():\n",
        "  print(\"\\t\", importance)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's display one of them:"
      ],
      "metadata": {
        "id": "USvNgqBR_JR2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI073gJHgHxr"
      },
      "outputs": [],
      "source": [
        "inspector.variable_importances()[\"NUM_AS_ROOT\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T75nMlpRc7mz"
      },
      "source": [
        "## Predict on a single example\n",
        "\n",
        "Here's example code you can use to make predictions on a single example. Note that TensorFlow is optimized for batch prediction. This code below is mainly helpful for experimenting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLXxNfsXYPS7"
      },
      "outputs": [],
      "source": [
        "# Create your example as a dictionary\n",
        "example = {\"bill_depth_mm\" : [0],\n",
        "           \"bill_length_mm\" : [0],\n",
        "           \"body_mass_g\" : [0],\n",
        "           \"flipper_length_mm\" : [0],\n",
        "           \"island\" : [\"Torgersen\"],\n",
        "           \"sex\" : \"female\",\n",
        "           \"year\" : 2007}\n",
        "\n",
        "# Convert the dictionary into a DataFrame\n",
        "example_df = pd.DataFrame.from_dict(example)\n",
        "\n",
        "# Convert the DataFrame into tf.data format\n",
        "example_ds = tfdf.keras.pd_dataframe_to_tf_dataset(example_df)\n",
        "\n",
        "# Call predict\n",
        "rf.predict(example_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YRjnYPwc1lj"
      },
      "source": [
        "## Predict on many examples\n",
        "\n",
        "Following is code you can use to display predictions for each example in the test set. Note that similar code will be a bit different for neural networks, which typically use a different data structure inside tf.data to pack the features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhaqythabYI9"
      },
      "outputs": [],
      "source": [
        "# Make predictions on every example in the test set\n",
        "predictions = rf.predict(test_ds)\n",
        "\n",
        "# Loop over the test set, and display the predicted value and label\n",
        "features, labels = next(iter(test_ds))\n",
        "for pred, label in zip(predictions, labels):\n",
        "  print (\"Pred:\", np.argmax(pred), \"Actual:\", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Gradient Boosted Trees\n",
        "\n",
        "In this exercise you will download the [census](https://archive.ics.uci.edu/ml/datasets/census+income) dataset which. This contains ~40K examples with a mix of numeric and categorical attributes. You will train a gradient boosted tree, identify important features, and evaluate your model's accuracy.\n",
        "\n",
        "We've provided a bunch of code you can use to explore the dataset, in case this is helpful to you in your future work. The code you need to write for this exercise is only a couple lines.\n",
        "\n",
        "Notes:\n",
        "- You can visualize this dataset in your browser using https://pair-code.github.io/facets/ \n",
        "- This dataset has fairness concerns, which you can learn about at https://www.tensorflow.org/responsible_ai\n",
        "\n",
        "### Instructions\n",
        "\n",
        "Complete the code cells below. See the comments for instructions. You can find a solution at the end."
      ],
      "metadata": {
        "id": "h2-4vFn2kSCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and explore the dataset"
      ],
      "metadata": {
        "id": "oHedYD3YCaXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!wget https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.data -O /tmp/adult.csv"
      ],
      "metadata": {
        "id": "4NRWwxOjXMs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the CSV\n",
        "!head /tmp/adult.csv"
      ],
      "metadata": {
        "id": "nanlkRImZQJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The CSV is missing a header\n",
        "cols = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
        "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'label'\n",
        "]\n",
        "\n",
        "df = pd.read_csv(\"/tmp/adult.csv\", names=cols)"
      ],
      "metadata": {
        "id": "OCmYzMxO895q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up\n",
        "\n",
        "# Drop a meaningless column\n",
        "df = df.drop(columns=['fnlwgt'])\n",
        "\n",
        "# Convert the label into integer format\n",
        "df[\"label\"] = df[\"label\"].apply(lambda x: 1 if x == ' >50K' else 0).values\n",
        "\n",
        "# Shuffle the dataset\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "m0qRrGpG3hHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info(verbose=True, show_counts=True)"
      ],
      "metadata": {
        "id": "M2aO04MpchgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_col = 'label'\n",
        "categorical_columns = list(df.select_dtypes(include='object').columns)\n",
        "numeric_columns = [c for c in df.columns if c not in categorical_columns]\n",
        "\n",
        "print('Categorical columns', categorical_columns)\n",
        "print('Numeric columns', numeric_columns)\n",
        "\n",
        "feature_columns = categorical_columns + numeric_columns"
      ],
      "metadata": {
        "id": "Fp8clnol4K0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = split_dataset(df)\n",
        "print(\"{} examples in training, {} examples in testing.\".format(\n",
        "    len(train_df), len(test_df)))"
      ],
      "metadata": {
        "id": "j5TFXHAf3xUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[numeric_columns].describe()"
      ],
      "metadata": {
        "id": "QQB9PvjJ4JX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[categorical_columns].nunique()"
      ],
      "metadata": {
        "id": "wAApEU-z4NGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in categorical_columns:\n",
        "  print(col, list(train_df[col].unique()))"
      ],
      "metadata": {
        "id": "3dioChz34a46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the class balance?"
      ],
      "metadata": {
        "id": "ZZEtOIgg7eA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"label\"].sum() / len(train_df)"
      ],
      "metadata": {
        "id": "o4kDNZk84fXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"label\"].sum() / len(test_df)"
      ],
      "metadata": {
        "id": "ZF4RqLUF4fxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train shape:', train_df.shape)\n",
        "print('Test shape :', test_df.shape)"
      ],
      "metadata": {
        "id": "Nr3KLRCLYvs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create tf.data.Datasets from the Pandas DataFrame, using the one-liner shown above."
      ],
      "metadata": {
        "id": "90KkJPxNa7Jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "# Add code to create a tf.data.Dataset for train and test from the DataFrames\n",
        "\n",
        "# Example...\n",
        "# train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(...\n",
        "# test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(..."
      ],
      "metadata": {
        "id": "CVhwsA0zXTl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create and train your model"
      ],
      "metadata": {
        "id": "Vx5UwlUCCeh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create your model. You can write a one-liner for this, similar to the example above. Previously, you learned how to work with Random Forests. If you like, you can create a Random Forest again, or you can try creating a Gradient Boosted Tree. \n",
        "\n",
        "As a reminder, you can see which models are available by running `tfdf.keras.get_all_models()`, or visiting [tensorflow.org/decision_forests](https://www.tensorflow.org/decision_forests)."
      ],
      "metadata": {
        "id": "M4QE2L38bRzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "# Add code to create a gradient boosted tree\n",
        "# Example ...\n",
        "# gbt = tfdf.keras. ...\n",
        "# gbt.compile(metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "HfT7KrdFkJAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train your model. You can write a one-liner for this, similar to the example above."
      ],
      "metadata": {
        "id": "xmLQLPtxkPIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "# Add code to train your model\n",
        "# Example ...\n",
        "# gbt.fit(..."
      ],
      "metadata": {
        "id": "MQnw48g5kOgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate your model\n",
        "\n",
        "Uncomment these cells after completing the code above."
      ],
      "metadata": {
        "id": "8PJ7yVhxmxxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#gbt.summary()"
      ],
      "metadata": {
        "id": "pt9QZcxNtJr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gbt.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "EoDaNgB6m2OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation = gbt.evaluate(x=test_ds,return_dict=True)\n",
        "\n",
        "# for name, value in evaluation.items():\n",
        "#   print(f\"{name}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "VKcodwmWtqxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspector = gbt.make_inspector()\n",
        "# inspector.evaluation()"
      ],
      "metadata": {
        "id": "nPMWQUQn5I7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspector.variable_importances()[\"NUM_AS_ROOT\"]"
      ],
      "metadata": {
        "id": "radLWEm75Ke5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfdf.model_plotter.plot_model_in_colab(gbt, tree_idx=0, max_depth=3)"
      ],
      "metadata": {
        "id": "JL-ChwmK5R0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution\n",
        "\n",
        "\n",
        "**Create tf.data.Datasets from the pd.DataFrame**\n",
        "\n",
        "To do so, you can write:\n",
        "\n",
        "```\n",
        "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"label\")\n",
        "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df, label=\"label\")\n",
        "```\n",
        "\n",
        "If you will be working with the same tf.data.Dataset multiple times, you can add `.cache()` at the end of those lines to keep it in memory.\n",
        "\n",
        "**Create and train a GradientBoostedTrees model**\n",
        "\n",
        "To do so, you can write:\n",
        "\n",
        "```\n",
        "gbt = tfdf.keras.GradientBoostedTreesModel()\n",
        "gbt.compile(metrics=[\"accuracy\"])\n",
        "gbt.fit(train_ds)\n",
        "```\n"
      ],
      "metadata": {
        "id": "oHRGTK6qksZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next steps\n",
        "\n",
        "**Try a larger dataset**\n",
        "\n",
        "Thanks to our friends at Kaggle, you can find a tabular dataset with ~1.7M rows and starter code for TensorFlow Decision Forests [here](https://www.kaggle.com/code/paultimothymooney/getting-started-with-tensorflow-decision-forests/). This is great if you'd like to start running larger experiments.\n",
        "\n",
        "**Hyperparameter tuning**\n",
        "\n",
        "You can use [Keras Tuner](https://keras.io/keras_tuner/) for easy hyperparameter optmization. "
      ],
      "metadata": {
        "id": "LoJev14jkb9k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_vvkD0hm7ly"
      },
      "source": [
        "# MNIST\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è0Ô∏è‚É£1Ô∏è‚É£2Ô∏è‚É£3Ô∏è‚É£4Ô∏è‚É£5Ô∏è‚É£6Ô∏è‚É£8Ô∏è‚É£9Ô∏è‚É£üîü"
      ],
      "metadata": {
        "id": "AUpIX-31LuGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training an image classifier on the MNIST dataset of handwritten digits is considered the \"hello world\" of computer vision. In this tutorial, you will download the dataset, then train a linear model, a neural network, and a deep neural network to classify it. \n",
        "\n",
        "**Key point:** Deep Learning is \"code light, but concept heavy\". You'll be able to implement a Deep Neural Network in about five lines of code, but the underlying concepts (cross-entropy, softmax, dense layers, etc) normally take a few months to learn. You do need to understand these all today to dive in."
      ],
      "metadata": {
        "id": "RzmIqfrYLtmX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06vnmM4FuWOx"
      },
      "source": [
        "## Import TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jciXPo_3C3Ct"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(\"You are using TensorFlow version\", tf.__version__)\n",
        "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "  print(\"You have a GPU enabled.\")\n",
        "else:\n",
        "  print(\"Enable a GPU before running this notebook.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2esj4IQFIRN"
      },
      "source": [
        "Colab has a variety of GPU types available (each new  instance is assigned one randomly, depending on availability). To see which type of GPU you have, you can run ```!nvidia-smi``` in a code cell. Some are quite fast!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnE-WS8Sn-gA"
      },
      "outputs": [],
      "source": [
        "# In this notebook, we'll use Keras: TensorFlow's user-friendly API to \n",
        "# define neural networks. Let's import Keras now.\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBxGBhTAo6tN"
      },
      "source": [
        "## Download the MNIST dataset\n",
        "MNIST contains 70,000 grayscale images in 10 categories. The images are low resolution (28 by 28 pixels). An important skill in Deep Learning is exploring your dataset, and understanding the format. Let's download MNIST, and explore it now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKvqxLsAo8fj"
      },
      "outputs": [],
      "source": [
        "dataset = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = dataset.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II_QfaijpOrv"
      },
      "source": [
        "There are 60,000 images in the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0VgO9P6pQRw"
      },
      "outputs": [],
      "source": [
        "print(train_images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn3EcT0ppRG6"
      },
      "source": [
        "And 10,000 in the testing set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VGb7BpFpSl9"
      },
      "outputs": [],
      "source": [
        "print(test_images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA9aCXKtpVs3"
      },
      "source": [
        "Each label is an integer between 0-9:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "johPrDjepW9c"
      },
      "outputs": [],
      "source": [
        "print(train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3M7RaIspX1j"
      },
      "source": [
        "## Preprocess the data\n",
        "The pixel values in the images range between 0 and 255. Let's normalize the values 0 and 1 by dividing all the images by 255. It's important that the training set and the testing set are preprocessed in the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PCe9-u4pg1J"
      },
      "outputs": [],
      "source": [
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d28eIDCqpiVQ"
      },
      "source": [
        "Let's display the first 25 images from the training set, and display the label below each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0GYYH_XpjuU"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(train_labels[i])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvfMnOtyqE7I"
      },
      "source": [
        "## Create the layers\n",
        "\n",
        "Neural networks are made up of layers. Here, you'll define the layers, and assemble them into a model. We will start with a single Dense layer. \n",
        "\n",
        "### What does a layer do?\n",
        "\n",
        "The basic building block of a neural network is the layer. Layers extract representations from the data fed into them. For example:\n",
        "\n",
        "- The first layer in a network might receives the pixel values as input. From these, it learns to detect edges (combinations of pixels). \n",
        "\n",
        "- The next layer in the network receives edges as input, and may learn to detect lines (combinations of edges). \n",
        "\n",
        "- If you added another layer, it might learn to detect shapes (combinations of edges).\n",
        "\n",
        "The \"Deep\" in \"Deep Learning\" refers to the depth of the network. Deeper networks can learn increasingly abstract patterns. Roughly, the width of a layer (in terms of the number of neurons) refers to the number of patterns it can learn of each type.\n",
        "\n",
        "Most of deep learning consists of chaining together simple layers. Most layers, such as [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), have parameters that are initialized randomly, then tuned (or learned) during training by gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta-amFtkr9DI"
      },
      "outputs": [],
      "source": [
        "# A linear model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc5N-mBCsJYS"
      },
      "source": [
        "The first layer in this network, [tf.keras.layers.Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten), transforms the format of the images from a two-dimensional array (of 28 by 28 pixels) to a one-dimensional array (of 28 * 28 = 784 pixels). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data. This is necessary since Dense layers require arrays as input.\n",
        "\n",
        "After the pixels are flattened, this model consists of a single Dense layer. This is a densely connected, or fully connected, neural layer. The Dense layer has 10 neurons with softmax activation. This returns an array of 10 probability scores that sum to 1. \n",
        "\n",
        "After classifying an image, each neuron will contains a score that indicates the probability that the current image belongs to one of the 10 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9npNSJYGsOs_"
      },
      "source": [
        "## Compile the model\n",
        "\n",
        "Before the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n",
        "\n",
        "*Loss function* ‚Äî This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
        "\n",
        "*Optimizer* ‚Äî This is how the model is updated based on the data it sees and its loss function.\n",
        "\n",
        "*Metrics* ‚Äî Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smPN2Oz1sP6f"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl710TxYsRN2"
      },
      "source": [
        "## Train the model\n",
        "Training the neural network model requires the following steps:\n",
        "\n",
        "1. Feed the training data to the model. In this example, the training data is in the ```train_images``` and ```train_labels``` arrays.\n",
        "\n",
        "1. The model learns to associate images and labels.\n",
        "\n",
        "1. You ask the model to make predictions about a test set‚Äîin this example, the ```test_images``` array.\n",
        "\n",
        "1. Verify that the predictions match the labels from the ```test_labels``` array.\n",
        "\n",
        "To begin training, call the ```model.fit``` method ‚Äî so called because it \"fits\" the model to the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snsjLq5vsTHv"
      },
      "outputs": [],
      "source": [
        "EPOCHS=10\n",
        "model.fit(train_images, train_labels, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT61pdPAsUgR"
      },
      "source": [
        "As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.90 (or 90%) on the training data. Accuracy may be slightly different each time you run this code, since the parameters inside the Dense layer are randomly initialized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BCjtzzmse3t"
      },
      "source": [
        "## Evaluate accuracy\n",
        "Next, compare how the model performs on the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMAOvcHnsgOb"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpVIYwY_shqM"
      },
      "source": [
        "It turns out that the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy represents overfitting. Overfitting is when a machine learning model performs worse on new, previously unseen inputs than on the training data. An overfitted model \"memorizes\" the training data‚Äîwith less accuracy on testing data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vCw5qU4skSI"
      },
      "source": [
        "## Make predictions\n",
        "With the model trained, you can use it to make predictions about some images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG3rn85Dslbz"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(test_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56liTfOzsnMa"
      },
      "source": [
        "Here, the model has predicted the label for each image in the testing set. Let's take a look at the first prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gNyhd-zsoYk"
      },
      "outputs": [],
      "source": [
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmJ884GFspZ_"
      },
      "source": [
        "A prediction is an array of 10 numbers. They represent the model's \"confidence\" that the image corresponds to each of the 10 digits. You can see which label has the highest confidence value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oVXIHafsqsO"
      },
      "outputs": [],
      "source": [
        "print(tf.argmax(predictions[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AE8NZjYsuSC"
      },
      "source": [
        "# Exercise: Fashion MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üëïüëüüëîüëó"
      ],
      "metadata": {
        "id": "P-JKTn9xrR9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above tutorial, you trained a linear model (a single Dense layer) on the MNIST dataset. As an exercise, let's modify your code above to:\n",
        "- Use a new dataset (Fashion MNIST)\n",
        "- Train a neural network (with two Dense layers, instead of just one)\n",
        "- Create plots to observe overfitting and underfitting"
      ],
      "metadata": {
        "id": "Ls7D-ZdZdaU9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kUMis2J3NT_"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "You will need to make two changes in the code above.\n",
        "\n",
        "**1) Import the Fashion MNIST** \n",
        "\n",
        "To do so, change the line\n",
        "\n",
        "```\n",
        "dataset = keras.datasets.mnist\n",
        "``` \n",
        "\n",
        "to \n",
        "\n",
        "```\n",
        "dataset = keras.datasets.fashion_mnist\n",
        "```\n",
        "\n",
        "**2) Modify the model definition to create a neural network**\n",
        "\n",
        "To do so, change the lines from:\n",
        "\n",
        "```\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "to\n",
        "\n",
        "```\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "This will define a neural network with a single hidden layer. If you like, you can experiment by adding a third Dense layer, which will create a deep neural network. For example:\n",
        "\n",
        "```\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "After making the above changes, on the Colab menu   select *Edit -> Clear all outputs* and *Runtime -> Restart runtime* to restore this notebook to a clean state. Run the cells in the tutorial above to train your neural network on Fashion MNIST.\n",
        "\n",
        "**3) Add plots to observe overfitting**\n",
        "\n",
        "If trained for too long, a NN may begin to memorize the training data (rather than learning patterns that generalize to unseen data). This is called overfitting. Of all the hyperparmeters in the design of your network (the number and width of layers, the optimizer, etc) - the most important to set properly is ```epochs```. You will learn more about this in exercise two.\n",
        "\n",
        "To create plots to observe overfitting, modify your training loop as follows.\n",
        "\n",
        "Change:\n",
        "\n",
        "```\n",
        "history = model.fit(train_images, train_labels, epochs=EPOCHS)\n",
        "```\n",
        "\n",
        "to:\n",
        "\n",
        "```\n",
        "history = model.fit(train_images, train_labels, \n",
        "                    validation_data=(test_images, test_labels),\n",
        "                    epochs=EPOCHS)\n",
        "```\n",
        "\n",
        "This will capture the accuracy and loss on the training and validation data after epoch. To plot the results, create a new code cell, and add the following code:\n",
        "\n",
        "```\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(EPOCHS)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQjV6NPRaWYi"
      },
      "source": [
        "# Game break: Teachable Machine\n",
        "If you'd like, now would be a good time to take a break from coding and try: https://teachablemachine.withgoogle.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n",
        "\n",
        "You don't always need to write models totally from scratch.\n",
        "\n",
        "With [transfer learning](https://developers.google.com/machine-learning/glossary#transfer-learning), you can take knowledge that a neural network has learned from one task and apply it to a different (but related) task.\n",
        "\n",
        "The way we do this is by using a _pre-trained model_. With transfer learning, you leverage the trained layers of the pre-trained model as a feature extraction, and add additional layers to fit your application.\n",
        "\n",
        "In this example, you'll use a model that has been trained on the ImageNet dataset. [ImageNet](https://www.image-net.org/) is a common image classification benchmark dataset that contains ~1M images of animals, furniture, clothing, and more.\n",
        "\n",
        "You'll use this pre-trained model to classify images from the TensorFlow Flowers dataset, which contains thousands of images of different flower types. You'll then fine-tune the pre-trained model for higher accuracy."
      ],
      "metadata": {
        "id": "0_YV7VsEE6R0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üåπ üå∏ üå∫ üåº üåª"
      ],
      "metadata": {
        "id": "4vyLFxbvFfqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll download the pre-trained model from [TensorFlow Hub](https://tfhub.dev/), which is a repository of pre-trained TensorFlow models.\n",
        "\n",
        "You'll learn how to:\n",
        "\n",
        "1. Use models from TensorFlow Hub with `tf.keras`.\n",
        "1. Use an image classification model from TensorFlow Hub.\n",
        "1. Do  transfer learning to fine-tune a model for your own image classes."
      ],
      "metadata": {
        "id": "0LBhkHfBFpbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import PIL.Image as Image\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "metadata": {
        "id": "4tVLxHh8FeqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Predictions from a pre-trained model"
      ],
      "metadata": {
        "id": "zNpSQ-ETFvUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the models in TensorFlow Hub contain the classification head. These models can be used without any retraining. The caveat is that they will only be able to make predictions for labels they have seen in the training set. So if your pretrained model was trained only on dog images, but you want to get predictions for flowers, it's not going to work well.\n",
        "\n",
        "Let's first define the path to the model in TensorFlow Hub that we want to use.\n",
        "\n",
        "Feel free to click the link below and read more about the model on the TF Hub site.\n",
        "\n",
        "We'll use [MobileNet V2](https://arxiv.org/abs/1704.04861), which is a computer vision model for mobile or embedded applications (think: running on your phone)."
      ],
      "metadata": {
        "id": "dmTwNhcEFx19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hub_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\""
      ],
      "metadata": {
        "id": "rawBRiIWFug_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we wrap this pre-trained model from TensorFlow Hub as a Keras layer with `hub.KerasLayer` and create a model using the `tf.keras.Sequential` API."
      ],
      "metadata": {
        "id": "0ItLElxJF_9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE = (224, 224)\n",
        "\n",
        "classifier = tf.keras.Sequential([\n",
        "    hub.KerasLayer(hub_model, input_shape=IMAGE_SHAPE+(3,))\n",
        "])"
      ],
      "metadata": {
        "id": "91rr0wsAGBDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download an image of Grace Hopper and get a prediction from this pre-trained MobileNet model."
      ],
      "metadata": {
        "id": "Go4cYpidGCVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grace_hopper = tf.keras.utils.get_file('image.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg')\n",
        "grace_hopper = Image.open(grace_hopper).resize(IMAGE_SHAPE)\n",
        "grace_hopper"
      ],
      "metadata": {
        "id": "6WGAwswmGDtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to scale the image by dividing by 255.\n",
        "Then we call the predict method on our model, passing in the image."
      ],
      "metadata": {
        "id": "2Ros8NIfGFdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grace_hopper = np.array(grace_hopper) / 255.0\n",
        "result = classifier.predict(grace_hopper[np.newaxis, ...])"
      ],
      "metadata": {
        "id": "AWrNv79UGG0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result is a 1001-element vector, rating the probability of each class for the image.\n",
        "\n",
        "The top class ID can be found with tf.math.argmax:"
      ],
      "metadata": {
        "id": "TByBcanmGJWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_class = tf.math.argmax(result[0], axis=-1)\n",
        "predicted_class.numpy()"
      ],
      "metadata": {
        "id": "ZOe66FilGJJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take the predicted_class ID (such as 653) and fetch the ImageNet dataset labels to decode the predictions:"
      ],
      "metadata": {
        "id": "byT0NzT1GL28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        "imagenet_labels = np.array(open(labels_path).read().splitlines())\n",
        "\n",
        "plt.imshow(grace_hopper)\n",
        "plt.axis('off')\n",
        "predicted_class_name = imagenet_labels[predicted_class]\n",
        "_ = plt.title(\"Prediction: \" + predicted_class_name.title())"
      ],
      "metadata": {
        "id": "lIaEs9YtGNx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Exercise: Try a different pre-trained model\n",
        "\n",
        "In the above tutorial, you used a pre-trained MobileNet model to get predictions for an imge. TensorFlow Hub contains *many* image models you can use.\n",
        "\n",
        "In this exercise, you'll modify the code to:\n",
        "- Use a different pre-trained model called [Inception V3](https://arxiv.org/abs/1409.4842)\n",
        "\n",
        "\n",
        "## Instructions\n",
        "\n",
        "You'll need to make one change in the code above.\n",
        "\n",
        "**1) Specify a different model URL** \n",
        "\n",
        "To do so, change the line\n",
        "\n",
        "```\n",
        "hub_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\"\n",
        "``` \n",
        "\n",
        "to \n",
        "\n",
        "```\n",
        "hub_model = \"https://tfhub.dev/google/imagenet/inception_v3/classification/5\"\n",
        "```\n",
        "\n",
        "You'll notice that even though the Inception model and the MobileNet model were both trained on the ImageNet dataset, they will produce different predictions for the Grace Hopper image.\n",
        "\n",
        "There are **many** models on TensorFlow Hub you can use for your image classification needs, including many state-of-the-art models. To use them, you generally need to change the model URL (as above), and use different image preprocessing (to convert your data into the format the model expects).\n",
        "\n",
        "Now let's continue and learn how to customize one of these models to work better for your domain. "
      ],
      "metadata": {
        "id": "x0vlHtEPGX76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add a classification head"
      ],
      "metadata": {
        "id": "zvyjzd9SHxRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the pre-trained model was fast, but not very helpful if we have labels that weren't included in the training set.\n",
        "\n",
        "However, we can still make use of the knowledge that the pre-trained model has learned from looking at the 1M+ images in the ImageNet dataset. To do this, we will add a classification layer to the pre-trained model, and then retrain just that final layer."
      ],
      "metadata": {
        "id": "-LaaJD5OH2xj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the dataset\n",
        "\n",
        "First, load this data into the model using the image data off disk with `tf.keras.utils.image_dataset_from_directory`."
      ],
      "metadata": {
        "id": "Y8Fr7zc4H4Rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_root = tf.keras.utils.get_file(\n",
        "  'flower_photos',\n",
        "  'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "   untar=True)"
      ],
      "metadata": {
        "id": "Ysm1jXhOHwi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  str(data_root),\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  str(data_root),\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "aireW3nbH9TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The flowers dataset has five classes:"
      ],
      "metadata": {
        "id": "5i2EIqfwH-rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = np.array(train_ds.class_names)\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "ICis8beeIA2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's scale our data by 255 using `tf.keras.layers.Rescaling`"
      ],
      "metadata": {
        "id": "2SFGOOV-IB-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x‚Äîimages, y‚Äîlabels.\n",
        "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x‚Äîimages, y‚Äîlabels."
      ],
      "metadata": {
        "id": "jzmMpP25IDmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check out the shape of the images and the labels"
      ],
      "metadata": {
        "id": "Wup8OHC2IE3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "b-yoIjn-IF5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we train the model, let's quickly see what the predictions look like on this Flowers dataset from the classifier we used in the previous section."
      ],
      "metadata": {
        "id": "eK3HY3Q0IHDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_batch = classifier.predict(train_ds)"
      ],
      "metadata": {
        "id": "_FTaxPrJIIMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_class_names = imagenet_labels[tf.math.argmax(result_batch, axis=-1)]\n",
        "predicted_class_names"
      ],
      "metadata": {
        "id": "9yzeV158IJHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,9))\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "for n in range(30):\n",
        "  plt.subplot(6,5,n+1)\n",
        "  plt.imshow(image_batch[n])\n",
        "  plt.title(predicted_class_names[n])\n",
        "  plt.axis('off')\n",
        "_ = plt.suptitle(\"ImageNet predictions\")"
      ],
      "metadata": {
        "id": "kCmU1hz2IJ91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are far from perfect, but reasonable considering that these are not the classes the model was trained for (except for \"daisy\").\n",
        "\n",
        "To improve our results, it's time to train on our data!\n",
        "\n",
        "### Download the headless model\n",
        "\n",
        "As before, we need to download the classifier. Notice that this time, the path does not contain _\"classification\"_. Instead it is a _\"feature vector\"_. \n",
        "\n",
        "TensorFlow Hub provides feature vectors, which are pre-trained models without the top classification layer. This is what we want if we plan to do any retraining."
      ],
      "metadata": {
        "id": "uIr7YnzmILdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor_model = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\""
      ],
      "metadata": {
        "id": "Ek1BZk_nIPNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, wrap the pre-trained model with `hub.KerasLayer`."
      ],
      "metadata": {
        "id": "q-ynXEjaITIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor_layer = hub.KerasLayer(\n",
        "    feature_extractor_model,\n",
        "    input_shape=(224, 224, 3),\n",
        "    trainable=False)"
      ],
      "metadata": {
        "id": "tBpAgIcYIVOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature extractor returns a 1280-element vector for each image (the image batch size remains at 32 in this example):"
      ],
      "metadata": {
        "id": "p5LnPMGHIWZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_batch = feature_extractor_layer(image_batch)\n",
        "print(feature_batch.shape)"
      ],
      "metadata": {
        "id": "Im7ubyJQIaB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, add a classification layer and create a model with the Keras Sequential API."
      ],
      "metadata": {
        "id": "zmbsGl1TIbEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(class_names)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  feature_extractor_layer,\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(num_classes)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "C3BUz3I-IcJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From `model.summary` you can see that the architecture of the model is the TF Hub model followed by a dense layer with 5 units (because we have 5 different types of flowers in our dataset).\n",
        "\n",
        "Next, we compile and fit our model. Make sure you're using a GPU or this could take a while..."
      ],
      "metadata": {
        "id": "ETSd1-ZYIdcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(),\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['acc'])"
      ],
      "metadata": {
        "id": "-sDrlDvCIl91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 10\n",
        "\n",
        "history = model.fit(train_ds,\n",
        "                    validation_data=val_ds,\n",
        "                    epochs=NUM_EPOCHS)"
      ],
      "metadata": {
        "id": "W1QzOH-7IqzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the predictions\n",
        "\n",
        "Obtain the ordered list of class names from the model predictions:"
      ],
      "metadata": {
        "id": "YL46uMTuIsB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_batch = model.predict(image_batch)\n",
        "predicted_id = tf.math.argmax(predicted_batch, axis=-1)\n",
        "predicted_label_batch = class_names[predicted_id]\n",
        "print(predicted_label_batch)"
      ],
      "metadata": {
        "id": "1XeqTlW2IuVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the model predictions:"
      ],
      "metadata": {
        "id": "Wotunq1OIvdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,9))\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "for n in range(30):\n",
        "  plt.subplot(6,5,n+1)\n",
        "  plt.imshow(image_batch[n])\n",
        "  plt.title(predicted_label_batch[n].title())\n",
        "  plt.axis('off')\n",
        "_ = plt.suptitle(\"Model predictions\")"
      ],
      "metadata": {
        "id": "jHep8KsbIwdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These labels look much more reasonable!\n",
        "\n",
        "### Next steps\n",
        "\n",
        "If you have a large dataset (say, of ~1M images) you can retrain the entire model you downloaded from TensorFlow Hub from scratch. This may give higher accuracy than using the pre-trained weights, but will take a while.\n",
        "\n",
        "```\n",
        "feature_extractor_layer = hub.KerasLayer(\n",
        "    feature_extractor_model,\n",
        "    input_shape=(224, 224, 3),\n",
        "    trainable=True)\n",
        "```"
      ],
      "metadata": {
        "id": "NJEfJK21Ix4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning more\n",
        "\n",
        "There are many resources on https://www.tensorflow.org/hub you can use to learn more. To start, you may like to check out the image classification [tutorial](https://www.tensorflow.org/hub/tutorials/image_classification) on TensorFlow Hub. If you're interested in NLP, you can check out the tutorials for [BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)."
      ],
      "metadata": {
        "id": "D06ntzXvHb6Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0B8TOWwCsbM"
      },
      "source": [
        "# Cats and Dogs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üêàüêïüê±üê∂"
      ],
      "metadata": {
        "id": "F-mpI6jfMN2j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob2E2ZIeSbDv"
      },
      "source": [
        " ## Download and explore the dataset\n",
        "\n",
        "Although you are downloading large files, you are doing so in Colab through Google Cloud Platform (instead of over your local WiFi connection). This means that downloads will usually be fast, regardless of your internet connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aPzoJnd_8QA"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7EocqtTHnnc"
      },
      "outputs": [],
      "source": [
        "# Our dataset is a zip on the web\n",
        "origin = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=origin, extract=True)\n",
        "path_to_folder = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlL8BiAkLn1E"
      },
      "source": [
        "The unzipped dataset has the following directory structure:\n",
        "\n",
        "<pre>\n",
        "<b>cats_and_dogs_filtered</b>\n",
        "|__ <b>train</b>\n",
        "    |______ <b>cats</b>: [cat.0.jpg, cat.1.jpg, cat.2.jpg ....]\n",
        "    |______ <b>dogs</b>: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]\n",
        "|__ <b>validation</b>\n",
        "    |______ <b>cats</b>: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ....]\n",
        "    |______ <b>dogs</b>: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrgB9RwpShf8"
      },
      "source": [
        "The dataset is divided into train and validation splits. Let's create variables that point to each of these directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4b5Fsw2Lj-P"
      },
      "outputs": [],
      "source": [
        "train_dir = os.path.join(path_to_folder, 'train')\n",
        "validation_dir = os.path.join(path_to_folder, 'validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG7DjqVkLlCY"
      },
      "outputs": [],
      "source": [
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj1dSkUlLs-m"
      },
      "source": [
        "Now let's count the number of images in each directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfwja3aILv3N"
      },
      "outputs": [],
      "source": [
        "num_cats_tr = len(os.listdir(train_cats_dir))\n",
        "num_dogs_tr = len(os.listdir(train_dogs_dir))\n",
        "\n",
        "num_cats_val = len(os.listdir(validation_cats_dir))\n",
        "num_dogs_val = len(os.listdir(validation_dogs_dir))\n",
        "\n",
        "total_train = num_cats_tr + num_dogs_tr\n",
        "total_val = num_cats_val + num_dogs_val\n",
        "\n",
        "print('Total training cat images:', num_cats_tr)\n",
        "print('Total training dog images:', num_dogs_tr)\n",
        "print('Total validation cat images:', num_cats_val)\n",
        "print('Total validation dog images:', num_dogs_val)\n",
        "print('---')\n",
        "print(\"Total training images:\", total_train)\n",
        "print(\"Total validation images:\", total_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbqGygJHvWd_"
      },
      "source": [
        "You should see that we have 3,000 total images (2,000 in train and 1,000 in validation). Note that this dataset is balanced (we have an equal number of cats and dogs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6HxxIkqSsj6"
      },
      "source": [
        "Tip: in addition to Python, you can run shell commands in Colab (for example, ```!ls $train_cats_dir```)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bOJXrOqSuoW"
      },
      "outputs": [],
      "source": [
        "!ls $train_cats_dir "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e_9iotCAH8E"
      },
      "source": [
        "Let's display a couple images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAjuM662viPY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGVX4U4RTeF6"
      },
      "outputs": [],
      "source": [
        "_ = plt.imshow(plt.imread(os.path.join(train_cats_dir, \"cat.0.jpg\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PM8Th5UAUOa"
      },
      "outputs": [],
      "source": [
        "_ = plt.imshow(plt.imread(os.path.join(train_cats_dir, \"cat.1.jpg\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYj7KnYUvj5v"
      },
      "source": [
        "Note that the images are different sizes. Before feeding them into a CNN, we'll need to reshape them all to the same dimensions. We'll take care of that in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1A2CLadVnQa"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5JfskKZL716"
      },
      "source": [
        "Next, we will need a way to read these images off disk, and to preprocess them. Specifically, we will need to:\n",
        "- Read the image off disk.\n",
        "- Decode contents of these images and convert them into RGB arrays.\n",
        "- Convert the pixels values from integer to floating point types.\n",
        "- Rescale the pixel from values between 0 and 255 to values between 0 and 1 (neural networks work better with small input values - under the hood, each input is multiplied by a weight, large inputs could result in overflow).\n",
        "\n",
        "Fortunately, all of these tasks can be done with the `ImageDataGenerator` class provided by `tf.keras`. It can read images from disk and preprocess them into proper arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbKVMUaBA14w"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVOP4BOcL57E"
      },
      "outputs": [],
      "source": [
        "# Let's resize images to this size\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEYTv3G6L9I2"
      },
      "outputs": [],
      "source": [
        "# Rescale the pixel values to range between 0 and 1\n",
        "train_generator = ImageDataGenerator(rescale=1./255)\n",
        "val_generator = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmM-kS68L-iY"
      },
      "source": [
        "After defining the generators for training and validation images, the `flow_from_directory` method load images from the disk, applies rescaling, and resizes the images into the required dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff0NXGJEVPWl"
      },
      "outputs": [],
      "source": [
        "batch_size = 32 # Read a batch of 64 images at each step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zenaWVaHL_jI"
      },
      "outputs": [],
      "source": [
        "train_data_gen = train_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                     directory=train_dir,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                     class_mode='binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbgDJgcUMAeY"
      },
      "outputs": [],
      "source": [
        "val_data_gen = val_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                 directory=validation_dir,\n",
        "                                                 target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                 class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h77qZjn2MFaZ"
      },
      "source": [
        "## Use the generators to display a few images and their labels\n",
        "\n",
        "Next, we will extract a batch of images from the training generator, then plot several of them with `matplotlib`. The `next` function returns a batch from the dataset. The return value of `next` function is in form of `(x_train, y_train)` where x_train is the pixel values and y_train is the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRkntEZ1MJev"
      },
      "outputs": [],
      "source": [
        "image_batch, labels_batch = next(train_data_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFItkxNsp1EN"
      },
      "outputs": [],
      "source": [
        "# The shape will be (32, 150, 150, 3)\n",
        "# This means a list of 32 images, each of which is 150x150x3.\n",
        "# The 3 at the end refers to the R,G,B color channels.\n",
        "# A grayscale image would be (for example) 150x150x1\n",
        "print(image_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6sb2tmtqEYE"
      },
      "outputs": [],
      "source": [
        "# The shape (32,) means a list of 64 numbers\n",
        "# each of these will either be 0 or 1\n",
        "print(labels_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4iqLgMjMMWs"
      },
      "outputs": [],
      "source": [
        "# This function will plot images returned by the generator\n",
        "# in a grid with 1 row and 5 columns\n",
        "def plot_images(images):\n",
        "  fig, axes = plt.subplots(1, 5, figsize=(10,10))\n",
        "  axes = axes.flatten()\n",
        "  for img, ax in zip(images, axes):\n",
        "      ax.imshow(img)\n",
        "      ax.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQUpgA1bMOti"
      },
      "outputs": [],
      "source": [
        "plot_images(image_batch[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnj3jfpvTlih"
      },
      "source": [
        "Next, let's retrieve the labels. All images will be labeled either 0 or 1, since this is a binary classification problem. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq1rm3jTTa9E"
      },
      "outputs": [],
      "source": [
        "# Here are the first 5 labels from the dataset\n",
        "# that correspond to the images above\n",
        "print(labels_batch[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuMd_qHITcxf"
      },
      "outputs": [],
      "source": [
        "# Here, we can see that \"0\" maps to cat,\n",
        "# and \"1\" maps to dog\n",
        "print(train_data_gen.class_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yukXvCdWMSDj"
      },
      "source": [
        "## Create the model\n",
        "Your model will consist of three convolutional blocks followed by max pooling. There's a fully connected layer with 256 units on top. This model will output class probabilities (between 0 and 1) based on the `sigmoid` activation function. If the output is closer to 1, the image will be classified as a dog, otherwise a cat. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAvbO8Q2BEN4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dd4nHoIMGy-"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, 3, padding='same', activation='relu', \n",
        "           input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN8hv9ItMXNr"
      },
      "source": [
        "Compile the model, and select the adam optimizer for gradient descent, and binary cross entropy for the loss function (roughly, cross entropy is a way to measure the distance between the prediction we wanted the network to make, and the prediction it made)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sarE21oqMYgC"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB2GwR_EMZwm"
      },
      "source": [
        "Let's look at a diagram of all the layers of the network using the `summary` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mgFrgh-MbL1"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGviR2YtBq_H"
      },
      "source": [
        "This model has about 5M parameters (or weights) to learn. Our model is ready to go, and next we can train it using the data generators we created earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx6po3HfMcTu"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmEm3VVwMeRg"
      },
      "source": [
        "Use the `fit` method to train the network. You will train the model for 15 epochs (an epoch is one \"sweep\" over the training set, where each image is used once to perform a round of gradient descent, and update the models parameters). This will take one to two minutes, so let's start it now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4ggEXVPWcxm"
      },
      "outputs": [],
      "source": [
        "epochs = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe5gKJ6eMfNt"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSjVPgaZ1bLn"
      },
      "source": [
        "Inside `model.fit`, TensorFlow uses gradient descent to find useful values for all the weights in the model. When you create the model, the weights are initialized randomly, then gradually improved over time. The data generator is used to load batches of data off disk. Then, for each batch:\n",
        "- The model performs a forward pass (the images are classified by the network).\n",
        "- Then, the model performs a backward pass (the error is computed, then each weight is slightly adjusted using gradient descent to improve the accuracy on the next iteration).\n",
        "\n",
        "Gradient descent is an iterative process. The longer you train the model, the more accurate it will become on the training set. But, the more likely it is to overfit! Meaning, the model will begin to memorize the training images, rather than learn patterns that enable it generalize to new images not included in the training set. \n",
        "\n",
        "- We can see whether overfitting is present by comparing the accuracy on the training and validation data.\n",
        "\n",
        "If you look at the accuracy figures reported above, you should see that training accuracy is over 90%, while validation accuracy is only around 70%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIYFh3m9MpvG"
      },
      "source": [
        "## Create plots to check for overfitting\n",
        "Accuracy on the validation data is important: it helps you estimate how well our model is likely to work on new, unseen data in the future. To see how much overfitting is present (and when it occurs), we will create two plots, one for accuracy, and another for loss. Roughly, loss (or error) is the inverse of accuracy (lower is better). Unlike accuracy, loss takes the confidence of a prediction into account (a confidently wrong predicitions has a higher loss than one that is only slightly wrong)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1ZuxB7iMrBG"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeBooB9-bpht"
      },
      "source": [
        "Overfitting occurs when the validation loss stops decreasing. In this case, that occurs around epoch 5 (give or take). Your results may be slightly different each time you run this code (since the weights are initialized randomly).\n",
        "\n",
        "Why does overfitting happen? When there are only a \"small\" number of training examples, the model sometimes learns from noises or unwanted details, to an extent that it negatively impacts the performance of the model on new examples. It means that the model will have a difficult time \"generalizing\" on a new dataset (making accurate predictions on images that weren't included in the training set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Tk_WFuauxw"
      },
      "source": [
        "# Game break: Quick, Draw!\n",
        "If you'd like, now would be a good time to take a break from coding and try: https://quickdraw.withgoogle.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3hfnxDST1we"
      },
      "source": [
        "# Exercise: Reduce overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-TD-ium4LJL"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "In this exercise, you will use data augmentation and dropout to improve your model. Follow along by reading and running the code below. There are two **TODOs** for you to complete, and a solution is given below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFrRJObuMv8d"
      },
      "source": [
        "## Data augmentation\n",
        "Overfitting occurs when there are a \"small\" number of training examples. One way to fix this problem is to increase the size of the training set, by gathering more data (the larger and more diverse the dataset, the better!)\n",
        "\n",
        "We can also use a technique called \"data augmentation\" to increase the size of the training set, by generating new examples from existing ones by applying random transformations (for example, rotation) that yield believable-looking images. \n",
        "\n",
        "This is especially effective when working with images. For example, our training set may only contain images of cats that are right side up. If our validation set contains images of cats that are upside down, our model may have trouble classifying them correctly. To help teach it that cats can appear in any orientation, we will randomly rotate images from our training set during training. This helps expose the model to more aspects of the data, and can lead to better generalization.\n",
        "\n",
        "Data augmentation is built into the ImageDataGenerator. You can specifiy different transformations, and it will take care of applying then during the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIWUljgzb5dw"
      },
      "outputs": [],
      "source": [
        "# Let's create new data generators, this time with \n",
        "# data augmentation enabled\n",
        "train_generator = ImageDataGenerator(\n",
        "                    rescale=1./255,\n",
        "                    rotation_range=45,\n",
        "                    width_shift_range=.15,\n",
        "                    height_shift_range=.15,\n",
        "                    horizontal_flip=True,\n",
        "                    zoom_range=0.5\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLl4-mjKb-vo"
      },
      "outputs": [],
      "source": [
        "train_data_gen = train_generator.flow_from_directory(batch_size=32,\n",
        "                                                     directory=train_dir,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                     class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXaXALsZ5CST"
      },
      "source": [
        "The next cell will show how the same training image appears when used with five different types of data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtLNwy9_b_RF"
      },
      "outputs": [],
      "source": [
        "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
        "plot_images(augmented_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsKX1NNpchmM"
      },
      "source": [
        "We only apply data augmentation to the training examples, so our validation generator looks the same as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZjPRpJkchwU"
      },
      "outputs": [],
      "source": [
        "val_generator = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQIyYxvAcknP"
      },
      "outputs": [],
      "source": [
        "val_data_gen = val_generator.flow_from_directory(batch_size=32,\n",
        "                                                 directory=validation_dir,\n",
        "                                                 target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                 class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-s-DCjFcm1N"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K34kcZfzco5M"
      },
      "source": [
        "Another technique to reduce overfitting is to introduce dropout to the network. Dropout is a form of regularization that makes it more difficult for the network to memorize rare details (instead, it is forced to learn more general patterns).\n",
        "\n",
        "When you apply dropout to a layer it randomly drops out (set to zero) a number of activations during training. Dropout takes a fractional number as its input value, in the form such as 0.1, 0.2, 0.4, etc. This means dropping out 10%, 20% or 40% of the output units randomly from the applied layer.\n",
        "\n",
        "When appling 0.1 dropout to a certain layer, it randomly deactivates 10% of the output units in each training epoch.\n",
        "\n",
        "Create a new model using Dropout. You'll reuse the model definition from above, and add a Dropout layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW2NXkclDxo6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3e7ir-HcnZ7"
      },
      "outputs": [],
      "source": [
        "# TODO: Your code here\n",
        "# Create a new CNN that takes advantage of Dropout.\n",
        "# 1) Reuse the model declared in tutorial above.\n",
        "# 2) Add a new line that says \"Dropout(0.2),\" immediately\n",
        "# before the line that says \"Flatten()\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hmzYnem6TEf"
      },
      "source": [
        "## Solution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnRSZBV86dy7"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "model = Sequential([\n",
        "    Conv2D(32, 3, padding='same', activation='relu', \n",
        "           input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Dropout(0.2),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMlmQwrwcu4V"
      },
      "source": [
        "After introducing dropout to the network, compile your model and view the layers summary. You should see a Dropout layer right before flatten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L77Pt-WFcxYz"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEMGlbwWc254"
      },
      "source": [
        "## Train your new model\n",
        "Add code to train your new model. Previously, we trained for 15 epochs. You will need to train this new modek for more epochs, as data augmentation and dropout make it more difficult for a CNN to memorize the training data (this is what we want!).\n",
        "\n",
        "Here, you'll train this model for 25 epochs. This may take a few minutes, and you may need to train it for longer to reach peak accuracy. If you like, you can continue experimenting with that at home."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jLoyPTRc2Va"
      },
      "outputs": [],
      "source": [
        "epochs = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BnRJWB164DY"
      },
      "outputs": [],
      "source": [
        "# TODO: your code here\n",
        "# Add code to call model.fit, using your new\n",
        "# data generators with image augmentation\n",
        "# For reference, see the \"Train the model\"\n",
        "# section above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n0HeqPt6_-1"
      },
      "source": [
        "## Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSwPqgkM7BU0"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7ktPjfbdCoC"
      },
      "source": [
        "## Evaluate your new model\n",
        "Finally, let's again create plots of accuracy and loss (we use these plots often in practice!) Now, compare the loss and accuracy curves for the training and validation data. Were you able to achieve a higher validation accuracy than before? Note that even this model will eventually overfit. To prevent that, we use a technique called early stopping (we stop training when the validation loss is no longer decreasing). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ma9lDmidMWs"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MsFn3y93rhS"
      },
      "source": [
        "# Game break: Sketch-RNN\n",
        "If you'd like, now would be a good time to take a break from coding and try: https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5cpeEEprlOT"
      },
      "source": [
        "# DeepDream"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõåüí≠"
      ],
      "metadata": {
        "id": "YQlkhkF0MTul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If time remains, in this tutorial your instructor will walk you through a minimal version of DeepDream, an experiment to visualize some of the features a convolutional neural network has learned to detect. DeepDream is an advanced tutorial, and our goal is to introduce you to some of the fascinating (and unexpected) things you can explore with Deep Learning. \n",
        "\n",
        "Normally, when training a model we use gradient descent to minimize classification loss. In a CNN, this means we adjust the weights in the filters. In DeepDream, we start with a large, pretrained CNN (and leave the filters fixed!) We then use gradient descent to modify the input image to increasingly activate the filters. For example, if there is a filter that recognizes a certain kind of texture, we can progressively modify the image to contain more and more examples of that texture."
      ],
      "metadata": {
        "id": "JcE_C7-qMQ7D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ5DZa1urWkH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzRZ1H47t20X"
      },
      "source": [
        "## Download and display an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-Ad_AGCrQvt"
      },
      "outputs": [],
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7I2qKIvrUyj"
      },
      "outputs": [],
      "source": [
        "def download(url, target_size=None):\n",
        "  name = url.split('/')[-1]\n",
        "  image_path = tf.keras.utils.get_file(name, origin=url)\n",
        "  return tf.keras.preprocessing.image.load_img(image_path, target_size)\n",
        "\n",
        "def show(img):\n",
        "  plt.figure(figsize=(8,8))\n",
        "  plt.grid(False)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "original_img = download(url, target_size=[225, 375])\n",
        "original_img = np.array(original_img)\n",
        "show(original_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNZadvf1t5Nx"
      },
      "source": [
        "## Rescale the pixel values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHZtAv0YraD5"
      },
      "outputs": [],
      "source": [
        "def preprocess(img):\n",
        "  \"\"\" Convert RGB values from [0, 255] to [-1, 1] \"\"\"\n",
        "  img = tf.cast(img, tf.float32)\n",
        "  img /= 128.0\n",
        "  img -= 1.\n",
        "  return img\n",
        "\n",
        "def unprocess(img):\n",
        "  \"\"\" Undo the preprocessing above \"\"\"\n",
        "  img = 255 * (img + 1.0) / 2.0\n",
        "  return tf.cast(img, tf.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD12eOvrt9wc"
      },
      "source": [
        "## Import a large, pretrained CNN\n",
        "This model has been trained on ImageNet, a dataset with about 1M images in about 1K classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyNR_ayNrcRJ"
      },
      "outputs": [],
      "source": [
        "conv_base = tf.keras.applications.InceptionV3(weights='imagenet', \n",
        "                                              include_top=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVrd2zgMuPPm"
      },
      "source": [
        "## Choose layers to activate\n",
        "Normally, when you train a neural network, you use gradient descent to adjust the weights to minimize loss, in order to accurately classify images. In DeepDream, the trick is to use gradient descent to adjust the **image**, in order to increasingly activate certain layers from the network. You can explore different layers and see how this affects the results. You can find all the layer names using ```model.summary()```. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRFucWpkreWb"
      },
      "outputs": [],
      "source": [
        "names = ['mixed2', 'mixed3', 'mixed4', 'mixed5']\n",
        "layers = [conv_base.get_layer(name).output for name in names]\n",
        "model = tf.keras.Model(inputs=conv_base.input, outputs=layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULXM15ysuqhj"
      },
      "source": [
        "## Custom loss function\n",
        "Normally, we would use cross-entropy loss (for classification), or mean squared error (for regression). Here, we'll write a loss function that describes how activated our layers were by the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAxeiGTCrgn6"
      },
      "outputs": [],
      "source": [
        "def calc_loss(img):\n",
        "  img_batch = tf.expand_dims(img, axis=0)\n",
        "  layer_activations = model(img_batch)\n",
        "  losses = [tf.math.reduce_mean(act) for act in layer_activations]\n",
        "  return tf.reduce_sum(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2CRKNrau4Zh"
      },
      "source": [
        "## Use gradient ascent to progressively activate the layers\n",
        "Normally, when training a model you use gradient *descent* to adjust the weights to reduce the loss. In DeepDream, you will use gradient *ascent* to maximize the activation of the layers you selected by modifying the image, while leaving the weights of the network fixed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B632smgOriZa"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def step(img, lr=0.001):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = calc_loss(img)\n",
        "\n",
        "  gradients = tape.gradient(loss, img)\n",
        "  gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
        "\n",
        "  # Because the gradients are in the same shape \n",
        "  # as the image, we can directly add them to it!\n",
        "  img.assign_add(gradients * lr)\n",
        "  img.assign(tf.clip_by_value(img, -1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnVu-oF0rlJZ"
      },
      "outputs": [],
      "source": [
        "img = tf.Variable(preprocess(original_img))\n",
        "\n",
        "steps = 1000\n",
        "for i in range(steps):\n",
        "  step(img)\n",
        "  if i % 200 == 0:\n",
        "    clear_output(wait=True)\n",
        "    print (\"Step {}\".format(i))\n",
        "    show(unprocess(img.numpy()))\n",
        "\n",
        "clear_output(wait=True)\n",
        "show(unprocess(img.numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8vF9au2vDnp"
      },
      "source": [
        "You can find a complete example on the [website](https://www.tensorflow.org/tutorials/generative/deepdream) (which includes additional code to generate less noisy images), and you may also be interested in exploring a related technique [Neural Style Transfer](https://www.tensorflow.org/tutorials/generative/style_transfer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JztBOYqZ6yTm"
      },
      "source": [
        "# Learning more"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìö"
      ],
      "metadata": {
        "id": "zca1iUzJMZa6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfSulmXVQyJp"
      },
      "source": [
        "Book recommendations (both of these are **excellent**):\n",
        "\n",
        "* [Deep Learning with Python, 2nd edition](https://www.manning.com/books/deep-learning-with-python-second-edition)\n",
        "* [Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thank you!"
      ],
      "metadata": {
        "id": "iuXY8INSMlfD"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "leap2022.ipynb",
      "provenance": [],
      "toc_visible": true,
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}